---
title: 'Machine Learning: Human Activity Recognition'
author: "Paul M. Washburn"
date: "September 2015"
output: html_document
---

# Overview
### Introduction
This is an assignment in the course Practical Machine Learning, which is course number 8 of 10 in Johns Hopkins Data Science Specialization. 

The [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) (HAR) dataset is a large compilation of metrics describing the movement patterns of certain individuals. 

### Background
Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in HAR with wearable accelerometers), especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.

Read More at: http://groupware.les.inf.puc-rio.br/har#ixzz3pVa0EBDJ

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

### Objectives:
  * Predict the manner in which exercises were performed ('classe')
    * May use any variable to predict with
  * Create a report on how model was built
    * How cross-validation was used
    * What your expectations were for out of sample error
    * Justify the choices made
  * Submit GitHub repo with R Markdown compiled for HTML
    * Text should be less than 2000 words 
    * Figures <= 5
  * Apply your machine learning algorithm to the 20 test cases available in the test data
    * Submit predictions in appropriate format; see the programming assignment for how to submit for automated grading
    
### Data Overview
The 5 classes of interest include: sitting down, standing up, standing, walking an dsitting.
  * 8 hours of activity on 4 healthy subjects per day
  * Baseline performance index is also established
  * This dataset is licensed under the Creative Commons license (CC BY-SA)
    
# Data Acquisition, Exploration and Feature Reduction
Load the libraries necessary and set the seed for reproducibility.
```{r}
set.seed(8008135)
library(caret)
library(corrplot)
```

Download the [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) datasets from their source and load into R. 
```{r}
setwd("C:/Users/Paul/Desktop/R")
path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#download.file(path, destfile="training.csv")
path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download.file(path, destfile="testing.csv")
trainingRaw <- read.csv("training.csv", header=TRUE, 
                     na.strings=c("", "NA", "NULL", "#DIV/0!"))
testingRaw <- read.csv("testing.csv", header=TRUE, 
                     na.strings=c("", "NA", "NULL", "#DIV/0!"))
```

Get to know the training dataset. 
```{r}
dim(trainingRaw)
dim(testingRaw)
```

The training dataset contains 19622 observations of 160 variables. Many of the variables have NA values, which we will exclude below.
```{r}
rmNaColumn <- function (x) { 
  !any(is.na(x) | x=="")
  }
clean <- trainingRaw[sapply(trainingRaw, rmNaColumn)]
cleanTestingForLater <- testingRaw[sapply(testingRaw, rmNaColumn)]
dim(clean)
dim(cleanTestingForLater)
```

The updated dataset has fewer variables than before since several columns were completely NA. Now partition the data set into training and test sets. Recall that classe is the outcome variable we are trying to predict. 
```{r}
inTrain <- createDataPartition(clean$classe, p=0.7, list=FALSE)
cleanTraining <- clean[inTrain,]
cleanTesting <- clean[-inTrain,]
dim(cleanTraining)
dim(cleanTesting)
```

Delete more unnecessary columns. 
```{r}
forDelete <- grepl("X|user_name|timestamp|new_window", colnames(cleanTraining))
cleanTraining <- cleanTraining[, !forDelete]
cleanTesting <- cleanTesting[, !forDelete]
cleanTestingForLater <- cleanTestingForLater[, !forDelete]
dim(cleanTraining)
dim(cleanTesting)
dim(cleanTestingForLater)
```

Since there are 53 variables let's visualize the the dataset using a dendrogram. This highlights the fact that there is a lot going on in this dataset, meaning we might be fitting the model to variables that don't matter at a great cost to computational efficiency.
```{r}
correl <- cor(cleanTraining[, -54])
plot(hclust(dist(correl)), cex = 0.7)
```


# Principal Components Analysis
We don't want a lot of multi-collinearity; let's use the handy findCorrelation function to filter out those variables. This leaves us with only 33 variables, which is much easier to deal with from a computational standpoint.
```{r}
highCollinearity <- findCorrelation(correl, cutoff=0.75)
cleanTraining <- cleanTraining[, -highCollinearity]
cleanTesting <- cleanTesting[, -highCollinearity]
cleanTestingForLater <- cleanTestingForLater[, -highCollinearity]
dim(cleanTraining)
dim(cleanTesting)
dim(cleanTestingForLater)
```

Finally we will check for near-zero covariates. Variables that have very little variability will likely be poor predictors. It appears all of the variables are not near-zero.
```{r}
nsv <- nearZeroVar(cleanTraining, saveMetrics=TRUE)
nsv
```


# Training A Model For Prediction
Now we will derive relationships from the remaining variables.

###### Random Forest Model
Here we will train a Random Forest model on the clean training dataset.
```{r}
library(randomForest)
control <- trainControl(allowParallel=T, method="cv", number=4)
model <- train(classe ~ ., data=cleanTraining, model='rf', trControl=control, 
               importance=TRUE)
predictions <- predict(model, newdata=cleanTesting)
```

We can check to see how good these first-pass predictions were.
```{r}
actuality <- cleanTesting$classe
numberRight <- sum(predictions == actuality)
numberTotal <- length(predictions)
percentAccuracy <- numberRight / numberTotal
percentAccuracy
```

The predictions were accurate 99.86% of the time. A confusion matrix is another way of looking at this outcome.
```{r}
confusionMatrix(cleanTesting$classe, predictions)$table
```

Now we can rank the principal components in a graphical format. The most important metrics are at the top going all the way down to the least important of the filtered down set of variables. 
```{r}
varImpPlot(model$finalModel, sort=TRUE, type=1, pch=18, col=1, cex=0.5,
           main='Relative Importance of Principal Components')
```
The top 6 variables are:

  * num_window
  * yaw_belt
  * pitch_forearm
  * magnet_dumbbell_z
  * magnet_belt_y
  * roll_dumbbell

So we will filter the training set and train a new model.

```{r}
colsThatMatter <- c('num_window', 'yaw_belt', 'pitch_forearm', 
                 'magnet_dumbbell_z', 'magnet_belt_y', 'roll_dumbbell',
                 'classe')
finalTraining <- cleanTraining[colsThatMatter]
```


# Cross-Validation Testing & Out-of-Sample Error Estimate
##### Using Principal Components, Re-Train Model & Assess Accuracy
```{r}
control <- trainControl(allowParallel=T, method="cv", number=4)
model <- train(classe ~ ., data=finalTraining, model='rf', trControl=control, 
               importance=TRUE)
predictions <- predict(model, newdata=cleanTesting)
```

Let's see how well these six predictors did in correctly predicting classe. 
```{r}
actuality <- cleanTesting$classe
numberRight <- sum(predictions == actuality)
numberTotal <- length(predictions)
percentAccuracy <- numberRight / numberTotal
percentAccuracy
```
We had a cost of -0.01% to our accuracy; the benefit was significantly decreasing the processing necessary in terms of features.

Out of sample error displayed below as just 0.19%. This is what we expect the error rate when using this model outside this dataset, which we are about to test.
```{r}
oosErrorEstimate <- 1 - percentAccuracy
oosErrorEstimate
```

# Predict classe On 20 Test Datasets
Below are the predictions on the 20 test datasets that were processed parallel to the others. 
```{r}
finalPredictions <- predict(model, newdata=cleanTestingForLater)
finalPredictions
```

Let's create the files to submit the predictions (1-20).
```{r}
answers <- list('B', 'A', 'B', 'A', 'A', 
                'E', 'D', 'B', 'A', 'A', 
                'B', 'C', 'B', 'A', 'E', 
                'E', 'A', 'B', 'B', 'B')

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```


